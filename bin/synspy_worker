#!/usr/bin/python

import json
from deriva_common import PollingErmrestCatalog, HatracStore, urlquote
from volspy.util import load_image
import os
import re
import sys
import platform
import time
import json
import datetime
import pytz
import atexit
import shutil
import tempfile
import subprocess
import urllib
import csv

# server to talk to... defaults to our own FQDN
servername = os.getenv('SYNSPY_SERVER', platform.uname()[1])

# secret session cookie
credfile = os.getenv('SYNSPY_CREDENTIALS', 'credentials.json')
credentials = json.load(open(credfile))

# handle to /dev/null we can use in Popen() calls below...
fnull = open(os.devnull, 'r+b')

# we want a temporary work space for our external processing scripts
startup_working_dir = os.getcwd()
working_dir = None

class WorkerRuntimeError (RuntimeError):
    pass

class WorkerNotReadyError (RuntimeError):
    pass

@atexit.register
def cleanup():
    """Delete temporary working dir and its contents.

       We want to run this on any exit including exception/crashes so
       we register it with runtime.

       We also call it during each major work cycle to reap space and
       then create a new temporary work area again.
    """
    global working_dir
    if working_dir is not None:
        sys.stderr.write('Purging working directory %s... ' % working_dir)
        os.chdir(startup_working_dir)
        shutil.rmtree(working_dir)
        sys.stderr.write('done.\n')
        working_dir = None

# these are peristent/logical connections so we create once and reuse
# they can retain state and manage an actual HTTP connection-pool
catalog = PollingErmrestCatalog(
    'https', 
    servername,
    '1',
    credentials
)

store = HatracStore(
    'https', 
    servername,
    credentials
)

### Custom work to run on job events...
def get_image(img_url):
    """Download image from URL returning base file name"""
    m = re.match('^(?P<basename>[^:]+)(?P<v>[:][0-9A-Z]+)?$', os.path.basename(img_url))
    img_filename = m.groupdict()['basename']
    store.get_obj(img_url, destfilename=img_filename)
    return img_filename

def get_image_grid(img_filename):
    """Process input image URL and return voxel grid spacing triple."""
    I, md = load_image(str(img_filename))
    return I.micron_spacing

def preprocess_roi(img_filename, subject_path, row):
    """Analyze ROI and upload resulting NPZ file, returning NPZ URL."""
    command = [ 'synspy-analyze', img_filename ]
    env = {
        'ZYX_SLICE': row['ZYX Slice'],
        'ZYX_IMAGE_GRID': '0.4,0.26,0.26',
        'SYNSPY_NUCLEI_DETECT': dict(nucleic='true').get(row['Segmentation Mode'], 'false'),
        'DUMP_PREFIX': './%s' % row['ID'],
    }
    sys.stderr.write('Using analysis environment %r\n' % (env,))
    analysis = subprocess.Popen(command, stdin=fnull, env=env)
    code = analysis.wait()
    del analysis
    if code != 0:
        raise WorkerRuntimeError('Non-zero analysis exit status %s!' % code)

    return store.put_loc(
        '%s/%s.npz' % (subject_path, row['ID']),
        '%s.npz' % row['ID'],
        headers={'Content-Type': 'application/octet-stream'}
    )

def filter_synspy_csv(csv_url):
    """Process input CSV URL and return filtered CSV filename."""
    m = re.match('^(?P<basename>.+)[.]csv(?P<v>[:][0-9A-Z]+)?$', os.path.basename(csv_url))
    base = m.groupdict()['basename']
    csv_filename = '%s.csv' % base
    
    # download the content to temp dir
    store.get_obj(csv_url, destfilename=csv_filename)

    # prepare to read CSV content from temp dir
    csv_file = open(csv_filename, 'r')
    reader = csv.DictReader(csv_file)
    
    # prepare to write filtered CSV to temp dir
    filtered_filename = '%s-only.csv' % base
    filtered_file = open(filtered_filename, 'w')
    writer = csv.writer(filtered_file)

    # write header
    writer.writerow(
        ('Z', 'Y', 'X', 'raw core', 'raw hollow', 'DoG core', 'DoG hollow')
        + ( ('red',) if 'red' in reader.fieldnames else ())
        + ('override',)
    )
    
    # copy w/ filtering
    for row in reader:
        if row['Z'] == 'saved' and row['Y'] == 'parameters' \
           or row['override'] and int(row['override']) == 7:
            writer.writerow(
                (row['Z'], row['Y'], row['X'], row['raw core'], row['raw hollow'], row['DoG core'], row['DoG hollow'])
                + ( (row['red'],) if 'red' in reader.fieldnames else ())
                + (row['override'],)
            )

    del reader
    csv_file.close()
    
    del writer
    filtered_file.close()
    
    return filtered_filename
    

def run_row_job(row):
    """Do idempotent filtering on cropped image record.

       Arguments:
         row: cropped image record from ERMrest

       Results: None

       Performs idempotent update of row in ERMrest if possible,
       also creates objects in Hatrac as needed.

    """
    global working_dir
    
    sys.stderr.write('Claimed job %s.\n' % row['ID'])

    working_dir = tempfile.mkdtemp(dir="/var/tmp")
    os.chdir(working_dir)
    sys.stderr.write('Using working directory %s.\n' % working_dir)

    try:
        # where we put stuff in hatrac
        subject_path = '/hatrac/Zf/%s' % row['Subject']

        updated_row = {}

        if row['Status'] is None and row['URL'] is not None:
            img_filename = get_image(row['URL'])

            if row['URL'] is not None and \
               (row['Z micron'] is None \
                or row['Y micron'] is None \
                or row['X micron'] is None):
                micron_spacing = get_image_grid(img_filename)
                updated_row.update({
                    'Z micron': float(micron_spacing[0]),
                    'Y micron': float(micron_spacing[1]),
                    'X micron': float(micron_spacing[2])
                })

            if row['Npz URL'] is None:
                updated_row['Npz URL'] = preprocess_roi(img_filename, subject_path, row)

            updated_row['Status'] = "analysis pending"

        if row['Segments URL'] is not None and row['Segments Filtered URL'] is None:
            segments_filtered_file = filter_synspy_csv(row['Segments URL'])
            segments_filtered_url = store.put_loc(
                '%s/%s' % (subject_path, segments_filtered_file),
                segments_filtered_file,
                headers={'Content-Type': 'text/csv'}
            )
            updated_row['Segments Filtered URL'] = segments_filtered_url
            updated_row['Status'] = "processed"
        elif row['Segments Filtered URL']:
            updated_row['Status'] = "processed"

        if updated_row:
            updated_row['ID'] = row['ID']
            
            # record analysis results in ermrest
            catalog.put(
                '/attributegroup/Zebrafish:Image%%20Region/ID;%s' % ','.join([
                    urlquote(col, safe='')
                    for col in list(updated_row.keys())
                    if col != 'ID'
                ]),
                json=[updated_row]
            )
            sys.stderr.write('\nupdated in ERMrest: %s' % json.dumps(updated_row, indent=2))
        else:
            raise WorkerRuntimeError('row has no work pending %s' % (row,))

        sys.stderr.write('Processing complete.\n')

    finally:
        sys.stderr.write('\n')
        cleanup()

# for state-tracking across look_for_work() iterations
idle_etag = None

def look_for_work():
    """Find, claim, and process experiment one record.

       1. Find row with actionable state (partial data and Status="analysis complete")
       2. Claim by setting Status="in progress"
       3. Download and process data as necessary
       4. Upload processing results to Hatrac
       5. Update ERMrest w/ processing result URLs and Status="processed"

       Do find/claim with HTTP opportunistic concurrency control and
       caching for efficient polling and quiescencs.

       On error, set Status="failed: reason"

       Result:
         true: there might be more work to claim
         false: we failed to find any work

    """
    global idle_etag

    # state machine:
    # Status is NULL and image URL is not NULL and Classifier is not NULL
    #  -> Z/Y/X micron set and Npz URL set and Status = "analysis pending"
    #  -> Status = "analysis complete"
    #  -> filtered URL set and Status = "processed"

    claimable_work_url = '/attribute/I:=Zebrafish:Image/!URL::null::/Zebrafish:Image%20Region/!Classifier::null::/Status::null::;Status=null;!Segments%20URL::null::&Segments%20Filtered%20URL::null::&Status=%22analysis%20complete%22/*,I:URL,I:Subject?limit=1'
    status_update_url = '/attributegroup/Zebrafish:Image%20Region/ID;Status'

    # this handled concurrent update for us to safely and efficiently claim a record
    idle_etag, batch = catalog.state_change_once(
        claimable_work_url,
        status_update_url,
        lambda row: {'ID': row['ID'], 'Status': "in progress"},
        idle_etag
    )

    # we used a batch size of 1 due to ?limit=1 above...
    for row, claim in batch:
        try:
            run_row_job(row)
        except Exception as e:
            # TODO: eat some exceptions and return True to continue?
            catalog.put(status_update_url, json=[{'ID': row['ID'], 'Status': "failed: %s" % e }])
            
            raise

        return True
    else:
        return False
        
catalog.blocking_poll(look_for_work)

